We started with No pre-processing , just tokenising, converting to sentences.. and then model embedding -  Model had convuliotn and Maxpooling but no droupout - (80 %-----)


Then, we added droupout, without preprciseeing - 94.

With preprocessing - 95.8

Fixing droupout parameter range(0.2(best), 0.3, 0.5) - improved by 1-2%.    0.5 was giving somewhere around 89%

increasing epochs, early stopping - with droupout 0.2 - 98.88


With trainable=True, - 97.97

with trainable = False - 98.65

One big problem - we made separate tokenisers for train and test which led to 2%. on fixing this error, accuracy went very high.

RNN

Embedding -> LSTM (100), No dropouts -> Dense(output layer with softmax). - 97.97

Added SpatialDropout layer - .2. added LSTM and Reccurrant dropout - 0.2 - 97.52

With bidirectional and dropout - 96.40

Bidirectional without dropout - 97.75

only with dropout 0.2 in Bidirectional - 91.91

LSTM 0.4 dropout bidirectional, trainable=True - 96.85

HAN
embedding (100, trainable = False), Bidirectional LSTM(100), Bidirectional LSTM(100), (MAX_SENT_LENGTH = 100, MAX_Sent = 40 )- 94.60

MAX_SENT_LENGTH = 50, MAX_Sent = 50 - 95.505

Without Bidirectional (both) - 63.37 :((

With Bidirectional, droupout for both = 0.2 = 97.52

now adding recurrent dropout = 0.2, - 95.05

only dropout for both = 0.35 - 98.20


FUTURE - 

increase embedding layers from 100 to 300
