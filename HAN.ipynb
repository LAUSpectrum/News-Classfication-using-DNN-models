{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachit-shah/adbi-project/blob/master/HAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BToPi2JX6jzd",
        "colab_type": "text"
      },
      "source": [
        "## Load Data and Glove Model From Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvS3pf0t6zsk",
        "colab_type": "code",
        "outputId": "c0a649aa-e791-4db4-a07f-5ed82e1c07fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9s7l5uI7QWJ",
        "colab_type": "code",
        "outputId": "f1dd7388-6ff1-4ebb-8caa-f8b969013a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout, LSTM, GRU, Bidirectional, SpatialDropout1D, TimeDistributed\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Flatten\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "from keras.models import Model\n",
        "from keras.initializers import Constant\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction import text \n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk.corpus\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from keras.layers import Embedding\n",
        "\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8c7Ay-J7Qwh",
        "colab_type": "code",
        "outputId": "242f4764-99a1-4608-95d1-5285625345fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!cp gdrive/'My Drive'/bbc-fulltext.zip .\n",
        "!cp gdrive/'My Drive'/glove.6B.zip .\n",
        "!unzip bbc-fulltext.zip > out.txt\n",
        "!rm bbc/README.TXT\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHySPyqdR9_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value= 123\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(seed_value)\n",
        "\n",
        "# 5. Configure a new global `tensorflow` session\n",
        "from keras import backend as K\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew62XD1v790p",
        "colab_type": "code",
        "outputId": "9d2ab0da-e11a-4bc1-bd93-caf4a5277425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "#Read Data\n",
        "categories = ['business','entertainment','politics','sport','tech']\n",
        "df = pd.DataFrame([],columns=['category','id','text'])\n",
        "for cat in categories:\n",
        "  for file in os.listdir(\"bbc/\"+cat):\n",
        "      if file.endswith(\".txt\"):\n",
        "          filepath = os.path.join(\"bbc/\"+cat, file)\n",
        "          text = open(filepath,'r', errors='ignore').read()\n",
        "          s = pd.Series([cat,int(filepath.split('/')[-1][:-4]),text],index=['category','id','text'])\n",
        "          df = df.append(s,ignore_index=True)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>business</td>\n",
              "      <td>324</td>\n",
              "      <td>Yukos seeks court action on sale\\n\\nYukos will...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>470</td>\n",
              "      <td>Saudi investor picks up the Savoy\\n\\nLondon's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>business</td>\n",
              "      <td>413</td>\n",
              "      <td>EC calls truce in deficit battle\\n\\nThe Europe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>business</td>\n",
              "      <td>185</td>\n",
              "      <td>US bank 'loses' customer details\\n\\nThe Bank o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>business</td>\n",
              "      <td>142</td>\n",
              "      <td>Asian banks halt dollar's slide\\n\\nThe dollar ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   category   id                                               text\n",
              "0  business  324  Yukos seeks court action on sale\\n\\nYukos will...\n",
              "1  business  470  Saudi investor picks up the Savoy\\n\\nLondon's ...\n",
              "2  business  413  EC calls truce in deficit battle\\n\\nThe Europe...\n",
              "3  business  185  US bank 'loses' customer details\\n\\nThe Bank o...\n",
              "4  business  142  Asian banks halt dollar's slide\\n\\nThe dollar ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZXIyNwn8tT3",
        "colab_type": "code",
        "outputId": "6ab6318b-3f2b-4777-b9cd-bc4e0e7b7b6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "'''\n",
        "#WordCloud\n",
        "cat = df['category'].unique()\n",
        "\n",
        "for i in range(0,len(cat)):\n",
        "    words = ' '.join(df.loc[df['category']==cat[i], 'text'])\n",
        "\n",
        "    wordcloud = WordCloud( \n",
        "                          stopwords=STOPWORDS,\n",
        "                          background_color='white',\n",
        "                          width=800,\n",
        "                          height=400\n",
        "                ).generate(words)\n",
        "    print(cat[i])\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "words = ' '.join(df.loc[:, 'text'])\n",
        "\n",
        "wordcloud = WordCloud( \n",
        "                      stopwords=STOPWORDS,\n",
        "                      background_color='white',\n",
        "                      width=800,\n",
        "                      height=400\n",
        "            ).generate(words)\n",
        "print(\"ALL Categories:\")\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#WordCloud\\ncat = df[\\'category\\'].unique()\\n\\nfor i in range(0,len(cat)):\\n    words = \\' \\'.join(df.loc[df[\\'category\\']==cat[i], \\'text\\'])\\n\\n    wordcloud = WordCloud( \\n                          stopwords=STOPWORDS,\\n                          background_color=\\'white\\',\\n                          width=800,\\n                          height=400\\n                ).generate(words)\\n    print(cat[i])\\n    plt.figure(figsize=(10, 5))\\n    plt.imshow(wordcloud)\\n    plt.axis(\\'off\\')\\n    plt.show()\\n\\nwords = \\' \\'.join(df.loc[:, \\'text\\'])\\n\\nwordcloud = WordCloud( \\n                      stopwords=STOPWORDS,\\n                      background_color=\\'white\\',\\n                      width=800,\\n                      height=400\\n            ).generate(words)\\nprint(\"ALL Categories:\")\\nplt.figure(figsize=(10, 5))\\nplt.imshow(wordcloud)\\nplt.axis(\\'off\\')\\nplt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR2fsHlbr_Tj",
        "colab_type": "text"
      },
      "source": [
        "## Label Encoder\n",
        "#### Use inverse_transform at the end after predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "385fGoXNldB0",
        "colab_type": "code",
        "outputId": "290a10b6-7e40-4cde-a72c-f44e402454c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "y = df['category']\n",
        "X = df.drop(['category','id'],axis=1)\n",
        "X_train, X_test, y_train, y_test  = train_test_split(X,y,stratify=y, test_size=0.2, random_state=123)\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "print('before: %s ...' %y_train[:5])\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(y_train)\n",
        "y_train = le.transform(y_train)\n",
        "\n",
        "print('after: %s ...' %y_train)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: 1508       sport\n",
            "1908        tech\n",
            "2104        tech\n",
            "2019        tech\n",
            "1245    politics\n",
            "Name: category, dtype: object ...\n",
            "after: [3 4 4 ... 1 0 2] ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9jqvQtYDhR0",
        "colab_type": "code",
        "outputId": "a3d739c0-4500-4d43-e3e5-aca21ef56945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "print(np.bincount(y_train))\n",
        "print(y.value_counts())\n",
        "print(le.inverse_transform([i for i in range(5)]))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[408 309 333 409 321]\n",
            "sport            511\n",
            "business         510\n",
            "politics         417\n",
            "tech             401\n",
            "entertainment    386\n",
            "Name: category, dtype: int64\n",
            "['business' 'entertainment' 'politics' 'sport' 'tech']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e6g_BjOsZ3i",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess Data - (split by \\n, remove periods, remove slashes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkDSvsY8cv5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parameters for hyperparameter  tuning\n",
        "dropout = 0.2\n",
        "EMBEDDING_DIM = 100\n",
        "traina = True #embeedding layer trainable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJwCB-hGlWXW",
        "colab_type": "code",
        "outputId": "cec3dc80-512c-4b03-c062-07ff39b6382a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_SENT_LENGTH = 50\n",
        "MAX_SENTS = 50\n",
        "MAX_NB_WORDS = 20000\n",
        "VALIDATION_SPLIT = 0.2\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.'+str(EMBEDDING_DIM)+'d.txt') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3Iv6MC3s26i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(X):\n",
        "  return X.map(lambda x: x.lower().split(\"\\n\")).map(lambda x: [y.split(\". \") for y in x]).map(lambda x: [i.replace('\\'','') for sl in x for i in sl if i is not ''])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EgQMq_CtK8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocess both train and test separately\n",
        "def replace_punct(st,punct):\n",
        "  for i in punct:\n",
        "    if i==\"..\":\n",
        "      st = st.replace(\"..\",'.')\n",
        "    elif i==\"--\" or i == '-':\n",
        "      st = st.replace(i,' ')\n",
        "    else:\n",
        "      st = st.replace(i,'')\n",
        "  return st\n",
        "punct = word_tokenize(string.punctuation) + ['``','...','..','\\'s','--','-','n\\'t','\\'','(',')','[',']','{','}']\n",
        "texts = preprocess(X_train['text']).map(lambda x: '. '.join(x)).map(lambda x: replace_punct(x,punct))\n",
        "test_text = preprocess(X_test['text']).map(lambda x: '. '.join(x)).map(lambda x: replace_punct(x,punct))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx7rLmbykkXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articles = texts.map(lambda x: sent_tokenize(x))\n",
        "test_articles = test_text.map(lambda x: sent_tokenize(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bVCPt-Bl0r_",
        "colab_type": "code",
        "outputId": "15adcd30-f7a7-4424-cea0-a810e44bd1cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "li = []\n",
        "se = []\n",
        "for x in articles:\n",
        "  li.append(len(x))\n",
        "  for y in x:\n",
        "    se.append(len(y.split()))\n",
        "  \n",
        "#plt.boxplot(li,showcaps=True)\n",
        "plt.boxplot(se,showcaps=True)\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': [<matplotlib.lines.Line2D at 0x7f883291da20>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7f8832946240>,\n",
              "  <matplotlib.lines.Line2D at 0x7f8832946588>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7f8832946c18>],\n",
              " 'means': [],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7f88329468d0>],\n",
              " 'whiskers': [<matplotlib.lines.Line2D at 0x7f883291db70>,\n",
              "  <matplotlib.lines.Line2D at 0x7f883291deb8>]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noJekijPoJaniDHpJqrj/B8naKy2BGI3FAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGvbU8VulRqC",
        "colab_type": "code",
        "outputId": "a2b8aa10-126c-403e-b069-2744600492d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
        "\n",
        "for i, sentences in enumerate(articles):\n",
        "    for j, sent in enumerate(sentences):\n",
        "        if j< MAX_SENTS:\n",
        "            wordTokens = text_to_word_sequence(sent)\n",
        "            k=0\n",
        "            for _, word in enumerate(wordTokens):\n",
        "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
        "                    data[i,j,k] = tokenizer.word_index[word]\n",
        "                    k=k+1\n",
        "                    \n",
        "word_index = tokenizer.word_index\n",
        "print('No. of %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of 28512 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12KRmMu8tK11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = np.zeros((len(test_text), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
        "\n",
        "for i, sentences in enumerate(test_articles):\n",
        "    for j, sent in enumerate(sentences):\n",
        "        if j< MAX_SENTS:\n",
        "            wordTokens = text_to_word_sequence(sent)\n",
        "            k=0\n",
        "            for _, word in enumerate(wordTokens):\n",
        "                if k<MAX_SENT_LENGTH and word in tokenizer.word_index and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
        "                    test_data[i,j,k] = tokenizer.word_index[word]\n",
        "                    k=k+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb3DxolUuYHy",
        "colab_type": "code",
        "outputId": "19441d97-f9ac-4e6a-9b27-19dcb8fef75f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_data.shape"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(445, 50, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmPomOGXoP0f",
        "colab_type": "code",
        "outputId": "9ecf6852-6af7-4f07-c19a-9d462cbf344e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "labels = to_categorical(np.asarray(y_train))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# split the data into a training set and a validation set\n",
        "np.random.seed(123)\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "xtrain = data[:-nb_validation_samples]\n",
        "ytrain = labels[:-nb_validation_samples]\n",
        "xval = data[-nb_validation_samples:]\n",
        "yval = labels[-nb_validation_samples:]"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (1780, 50, 50)\n",
            "Shape of label tensor: (1780, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wme2RkYu2UR0",
        "colab_type": "code",
        "outputId": "2ef24db7-6912-494f-d6e6-40e73317672c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "'''\n",
        "#Stop Words and Lemmatization   \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stopw = nltk.corpus.stopwords.words('english')\n",
        "punct = string.punctuation\n",
        "punct = word_tokenize(punct)\n",
        "punct += ['.','``','...','\\'s','--','-','n\\'t','\\'']\n",
        "stopw += punct\n",
        "def token_stop(text):\n",
        "    global stopw\n",
        "    global lemmatizer\n",
        "    words = word_tokenize(text)\n",
        "    filtered = [lemmatizer.lemmatize(w) for w in words if not w in stopw]\n",
        "    return filtered\n",
        "  \n",
        "X_train['text'] = X_train['text'].map(lambda x: [token_stop(i) for i in x]).map(lambda x: [i for sl in x for i in sl])\n",
        "X_test['text'] = X_test['text'].map(lambda x: [token_stop(i) for i in x]).map(lambda x: [i for sl in x for i in sl])\n",
        "'''"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#Stop Words and Lemmatization   \\nlemmatizer = WordNetLemmatizer()\\nstopw = nltk.corpus.stopwords.words('english')\\npunct = string.punctuation\\npunct = word_tokenize(punct)\\npunct += ['.','``','...',''s','--','-','n't',''']\\nstopw += punct\\ndef token_stop(text):\\n    global stopw\\n    global lemmatizer\\n    words = word_tokenize(text)\\n    filtered = [lemmatizer.lemmatize(w) for w in words if not w in stopw]\\n    return filtered\\n  \\nX_train['text'] = X_train['text'].map(lambda x: [token_stop(i) for i in x]).map(lambda x: [i for sl in x for i in sl])\\nX_test['text'] = X_test['text'].map(lambda x: [token_stop(i) for i in x]).map(lambda x: [i for sl in x for i in sl])\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VofHdiAqIRqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create glove embedding matrix\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuZyVOsKKD6T",
        "colab_type": "code",
        "outputId": "9b9fbcaf-9b3a-4b46-cb10-4334a9a4d6ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Map label name to its index\n",
        "labels_index = {}\n",
        "for i in range(5):\n",
        "  name = le.inverse_transform([i])[0]\n",
        "  labels_index[name] = i\n",
        "labels_index"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e18TBVrC9iwT",
        "colab_type": "text"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xOvfu82wTmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Keras Glove Embedding layer\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SENT_LENGTH,\n",
        "                            trainable=traina)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijGeDWg6-XsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = \"HAN-\"+str(dropout)+\"-\"+str(EMBEDDING_DIM)+\"-Trainable-\"+str(traina)\n",
        "checkpointer = ModelCheckpoint(model_name + \"_weights.{epoch:02d}-{val_loss:.2f}.hdf5\", monitor=\"val_loss\", verbose=1,\n",
        "                               save_best_only=True, mode='min')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "tensorboard_han = TensorBoard(log_dir='./Graph_HAN', histogram_freq=1,write_graph=True,write_grads=True, write_images=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53s9TZ_mI_XY",
        "colab_type": "code",
        "outputId": "0cfefc38-59dd-47e3-87c5-ca75a5f50578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sentence_input)\n",
        "lstm_layer = Bidirectional(LSTM(100,dropout=dropout))(embedded_sequences)\n",
        "sentEncoder = Model(sentence_input, lstm_layer)\n",
        "\n",
        "article_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
        "article_encoder = TimeDistributed(sentEncoder)(article_input)\n",
        "lstm_layer_sent = Bidirectional(LSTM(100,dropout=dropout))(article_encoder)\n",
        "preds = Dense(len(labels_index), activation='softmax')(lstm_layer_sent)\n",
        "han_model = Model(article_input, preds)\n",
        "\n",
        "han_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "han_model.summary()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, 50, 50)            0         \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 50, 200)           8874700   \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 200)               240800    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 1005      \n",
            "=================================================================\n",
            "Total params: 9,116,505\n",
            "Trainable params: 9,116,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp3SQDq3-tfj",
        "colab_type": "code",
        "outputId": "fb64aa95-3070-4918-a92c-ec6e762aff9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967
        }
      },
      "source": [
        "stmillis = int(round(time.time() * 1000))\n",
        "history = han_model.fit(xtrain, ytrain, validation_data=(xval, yval),\n",
        "          epochs=50, batch_size=128, callbacks=[checkpointer, early_stopping])\n",
        "endmillis = int(round(time.time() * 1000))\n",
        "print(\"Time taken: \", endmillis - stmillis)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1424 samples, validate on 356 samples\n",
            "Epoch 1/50\n",
            "1424/1424 [==============================] - 14s 10ms/step - loss: 1.1954 - acc: 0.5885 - val_loss: 0.7857 - val_acc: 0.7640\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.78569, saving model to HAN-0.2-300-Trainable-True_weights.01-0.79.hdf5\n",
            "Epoch 2/50\n",
            "1424/1424 [==============================] - 10s 7ms/step - loss: 0.4329 - acc: 0.8729 - val_loss: 0.1689 - val_acc: 0.9466\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.78569 to 0.16888, saving model to HAN-0.2-300-Trainable-True_weights.02-0.17.hdf5\n",
            "Epoch 3/50\n",
            "1424/1424 [==============================] - 10s 7ms/step - loss: 0.2140 - acc: 0.9417 - val_loss: 0.1541 - val_acc: 0.9607\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.16888 to 0.15408, saving model to HAN-0.2-300-Trainable-True_weights.03-0.15.hdf5\n",
            "Epoch 4/50\n",
            "1424/1424 [==============================] - 10s 7ms/step - loss: 0.1334 - acc: 0.9614 - val_loss: 0.0729 - val_acc: 0.9775\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.15408 to 0.07288, saving model to HAN-0.2-300-Trainable-True_weights.04-0.07.hdf5\n",
            "Epoch 5/50\n",
            "1424/1424 [==============================] - 10s 7ms/step - loss: 0.1192 - acc: 0.9649 - val_loss: 0.0687 - val_acc: 0.9803\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.07288 to 0.06865, saving model to HAN-0.2-300-Trainable-True_weights.05-0.07.hdf5\n",
            "Epoch 6/50\n",
            "1424/1424 [==============================] - 10s 7ms/step - loss: 0.0559 - acc: 0.9860 - val_loss: 0.4042 - val_acc: 0.8652\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.06865\n",
            "Epoch 7/50\n",
            "1424/1424 [==============================] - 10s 7ms/step - loss: 0.0891 - acc: 0.9740 - val_loss: 0.0648 - val_acc: 0.9803\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.06865 to 0.06478, saving model to HAN-0.2-300-Trainable-True_weights.07-0.06.hdf5\n",
            "Epoch 8/50\n",
            "1424/1424 [==============================] - 10s 7ms/step - loss: 0.0964 - acc: 0.9761 - val_loss: 0.0473 - val_acc: 0.9860\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.06478 to 0.04732, saving model to HAN-0.2-300-Trainable-True_weights.08-0.05.hdf5\n",
            "Epoch 9/50\n",
            "1424/1424 [==============================] - 10s 7ms/step - loss: 0.0442 - acc: 0.9874 - val_loss: 0.0459 - val_acc: 0.9803\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.04732 to 0.04586, saving model to HAN-0.2-300-Trainable-True_weights.09-0.05.hdf5\n",
            "Epoch 10/50\n",
            "1424/1424 [==============================] - 10s 7ms/step - loss: 0.0283 - acc: 0.9909 - val_loss: 0.0329 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.04586 to 0.03286, saving model to HAN-0.2-300-Trainable-True_weights.10-0.03.hdf5\n",
            "Epoch 11/50\n",
            "1424/1424 [==============================] - 10s 7ms/step - loss: 0.0088 - acc: 0.9993 - val_loss: 0.0812 - val_acc: 0.9803\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.03286\n",
            "Epoch 12/50\n",
            "1424/1424 [==============================] - 10s 7ms/step - loss: 0.0855 - acc: 0.9719 - val_loss: 0.0293 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.03286 to 0.02931, saving model to HAN-0.2-300-Trainable-True_weights.12-0.03.hdf5\n",
            "Epoch 13/50\n",
            "1424/1424 [==============================] - 10s 7ms/step - loss: 0.0506 - acc: 0.9860 - val_loss: 0.0423 - val_acc: 0.9831\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.02931\n",
            "Epoch 14/50\n",
            " 384/1424 [=======>......................] - ETA: 6s - loss: 0.0102 - acc: 1.0000"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z73Z42jPBsd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig=plt.figure()\n",
        "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
        "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
        "plt.legend(['Training Loss', 'Validation Loss'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.title('Loss Curves : HAN',fontsize=16)\n",
        "fig.savefig(model_name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUmI9aHUlCgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nUIMMQi_KrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the trained weights\n",
        "han_model.save(model_name + \".h5\")\n",
        "\n",
        "# Save model config as json\n",
        "model_json = han_model.to_json()\n",
        "with open(model_name + \".json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Persist the model to your google drive [VERY IMPORTANT]\n",
        "!cp HAN* gdrive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja5gH7GZvxWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BAwr4x19nMo",
        "colab_type": "text"
      },
      "source": [
        "## Predict on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIBlzM5zFVhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_test_acc(prediction):\n",
        "  count = 0\n",
        "  t = 0\n",
        "  for pred in prediction:\n",
        "    p = pred.argmax()\n",
        "    name = le.inverse_transform([p])\n",
        "    #print(count)\n",
        "    #print(y_test.loc[count])\n",
        "    if name == y_test.loc[count]:\n",
        "      t+=1\n",
        "    count+=1\n",
        "  print('Test Accuracy:',(t/count)*100,\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sM8QeKfJgRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_test(model):\n",
        "  prediction = model.predict(test_data)\n",
        "  find_test_acc(prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_z5eMImMj57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_test(han_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgYSOq18cH1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R55FX-Xq3XkP",
        "colab_type": "text"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz0qLhb1Gi2s",
        "colab_type": "code",
        "outputId": "f9b769a1-2122-42d3-d4cb-ecba17e138fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-02 00:26:01--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.202.60.111, 52.72.245.79, 52.7.169.168, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.202.60.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14991793 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.30M  62.3MB/s    in 0.2s    \n",
            "\n",
            "2019-05-02 00:26:02 (62.3 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [14991793/14991793]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VIBupfd29Rk",
        "colab_type": "code",
        "outputId": "8f60e7e6-c127-4544-9331-8b1ddf6e4681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "LOG_DIR = './Graph_HAN'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://f759b158.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMMIu1W83CFi",
        "colab_type": "code",
        "outputId": "c011f09c-094b-4b26-ffa4-4817f9283e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_cnn.png\t  HAN_weights.08-0.25.hdf5\n",
            "bbc\t\t\t  HAN_weights.08-1.18.hdf5\n",
            "bbc-fulltext.zip\t  HAN_weights.09-0.17.hdf5\n",
            "gdrive\t\t\t  HAN_weights.09-1.14.hdf5\n",
            "glove.6B.100d.txt\t  HAN_weights.11-0.16.hdf5\n",
            "glove.6B.200d.txt\t  HAN_weights.11-0.22.hdf5\n",
            "glove.6B.300d.txt\t  HAN_weights.11-1.12.hdf5\n",
            "glove.6B.50d.txt\t  HAN_weights.12-0.15.hdf5\n",
            "glove.6B.zip\t\t  HAN_weights.12-0.19.hdf5\n",
            "HAN.h5\t\t\t  HAN_weights.12-1.10.hdf5\n",
            "HAN.json\t\t  HAN_weights.13-0.13.hdf5\n",
            "HAN_weights.01-0.82.hdf5  HAN_weights.13-0.15.hdf5\n",
            "HAN_weights.01-0.92.hdf5  HAN_weights.14-0.12.hdf5\n",
            "HAN_weights.01-0.95.hdf5  HAN_weights.14-0.13.hdf5\n",
            "HAN_weights.01-1.61.hdf5  HAN_weights.16-0.13.hdf5\n",
            "HAN_weights.02-0.45.hdf5  HAN_weights.16-1.02.hdf5\n",
            "HAN_weights.02-0.47.hdf5  HAN_weights.19-0.92.hdf5\n",
            "HAN_weights.02-0.58.hdf5  HAN_weights.23-0.12.hdf5\n",
            "HAN_weights.02-1.59.hdf5  HAN_weights.23-0.91.hdf5\n",
            "HAN_weights.03-0.53.hdf5  HAN_weights.24-0.10.hdf5\n",
            "HAN_weights.04-0.32.hdf5  HAN_weights.25-0.10.hdf5\n",
            "HAN_weights.04-0.44.hdf5  HAN_weights.30-0.09.hdf5\n",
            "HAN_weights.04-1.58.hdf5  HAN_weights.31-0.89.hdf5\n",
            "HAN_weights.05-0.29.hdf5  HAN_weights.33-0.86.hdf5\n",
            "HAN_weights.05-0.30.hdf5  HAN_weights.34-0.82.hdf5\n",
            "HAN_weights.05-1.46.hdf5  ngrok\n",
            "HAN_weights.06-0.31.hdf5  ngrok-stable-linux-amd64.zip\n",
            "HAN_weights.06-1.32.hdf5  out.txt\n",
            "HAN_weights.07-0.19.hdf5  sample_data\n",
            "HAN_weights.08-0.16.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxpBpPCy3Om5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}